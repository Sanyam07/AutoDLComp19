% Encoding: ISO-8859-1

@InProceedings{han18,
  author    = {Yihui He and Ji Lin and Zhijian Liu and Hanrui Wang and Li{-}Jia Li and Song Han},
  title     = {{AMC:} AutoML for Model Compression and Acceleration on Mobile Devices},
  booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part {VII}},
  year      = {2018},
  pages     = {815--832},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/eccv/HeLLWLH18},
  crossref  = {DBLP:conf/eccv/2018-7},
  doi       = {10.1007/978-3-030-01234-2\_48},
  timestamp = {Mon, 18 Nov 2019 15:06:18 +0100},
  url       = {https://doi.org/10.1007/978-3-030-01234-2\_48},
}

@InProceedings{finn17,
  author    = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1126--1135},
  address   = {International Convention Centre, Sydney, Australia},
  publisher = {PMLR},
  abstract  = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  pdf       = {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url       = {http://proceedings.mlr.press/v70/finn17a.html},
}

@Article{han15,
  author      = {Han, Song and Mao, Huizi and Dally, William J.},
  title       = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  year        = {2015},
  note        = {cite arxiv:1510.00149Comment: Published as a conference paper at ICLR 2016 (oral)},
  abstract    = {Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems with limited hardware
resources. To address this limitation, we introduce "deep compression", a three
stage pipeline: pruning, trained quantization and Huffman coding, that work
together to reduce the storage requirement of neural networks by 35x to 49x
without affecting their accuracy. Our method first prunes the network by
learning only the important connections. Next, we quantize the weights to
enforce weight sharing, finally, we apply Huffman coding. After the first two
steps we retrain the network to fine tune the remaining connections and the
quantized centroids. Pruning, reduces the number of connections by 9x to 13x;
Quantization then reduces the number of bits that represent each connection
from 32 to 5. On the ImageNet dataset, our method reduced the storage required
by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method
reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of
accuracy. This allows fitting the model into on-chip SRAM cache rather than
off-chip DRAM memory. Our compression method also facilitates the use of
complex neural networks in mobile applications where application size and
download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,
compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy
efficiency.},
  added-at    = {2017-06-03T00:10:05.000+0200},
  biburl      = {https://www.bibsonomy.org/bibtex/20a862efa8c33bda34d56dbe23a07a8e1/hprop},
  description = {[1510.00149] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  interhash   = {2ba770eb4cda1bfa599aecfc6fdd2914},
  intrahash   = {0a862efa8c33bda34d56dbe23a07a8e1},
  keywords    = {compression distillation machine-learning},
  timestamp   = {2017-06-03T00:10:05.000+0200},
  url         = {http://arxiv.org/abs/1510.00149},
}

@Article{hinton15,
  author    = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  title     = {Distilling the knowledge in a neural network},
  journal   = {arXiv preprint arXiv:1503.02531},
  year      = {2015},
  added-at  = {2019-02-21T13:17:32.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/2f99e2c9f4da7a46e2a33e12b639ea8aa/loroch},
  interhash = {67f341022f752a5833b5c6c35903c111},
  intrahash = {f99e2c9f4da7a46e2a33e12b639ea8aa},
  keywords  = {deep_learning distilling reduction},
  timestamp = {2019-02-21T13:17:32.000+0100},
  url       = {https://arxiv.org/abs/1503.02531v1},
}

@InProceedings{phuong19,
  author    = {Phuong, Mary and Lampert, Christoph},
  title     = {Towards Understanding Knowledge Distillation},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {5142--5151},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/phuong19a/phuong19a.pdf},
  url       = {http://proceedings.mlr.press/v97/phuong19a.html},
}

@InProceedings{reagan18,
  author    = {Reagan, Brandon and Gupta, Udit and Adolf, Bob and Mitzenmacher, Michael and Rush, Alexander and Wei, Gu-Yeon and Brooks, David},
  title     = {Weightless: Lossy weight encoding for deep neural network compression},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  pages     = {4324--4333},
  month     = {10--15 Jul},
  publisher = {PMLR},
  abstract  = {The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496x without loss of model accuracy. This results in up to a 1.51x improvement over the state-of-the-art.},
  pdf       = {http://proceedings.mlr.press/v80/reagan18a/reagan18a.pdf},
  url       = {http://proceedings.mlr.press/v80/reagan18a.html},
}

@Article{pan10,
  author     = {Pan, Sinno Jialin and Yang, Qiang},
  title      = {A Survey on Transfer Learning},
  journal    = {IEEE Trans. on Knowl. and Data Eng.},
  year       = {2010},
  volume     = {22},
  number     = {10},
  pages      = {1345--1359},
  month      = oct,
  issn       = {1041-4347},
  acmid      = {1850545},
  address    = {Piscataway, NJ, USA},
  doi        = {10.1109/TKDE.2009.191},
  issue_date = {October 2010},
  keywords   = {Transfer learning, data mining., machine learning, survey},
  numpages   = {15},
  publisher  = {IEEE Educational Activities Department},
  url        = {https://doi.org/10.1109/TKDE.2009.191},
}

@Book{hutter19,
  title     = {Automatic Machine Learning: Methods, Systems, Challenges},
  publisher = {Springer},
  year      = {2018},
  editor    = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  note      = {In press, available at http://automl.org/book.},
}

@InProceedings{santoro16,
  author    = {Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
  title     = {Meta-Learning with Memory-Augmented Neural Networks},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1842--1850},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
  pdf       = {http://proceedings.mlr.press/v48/santoro16.pdf},
  url       = {http://proceedings.mlr.press/v48/santoro16.html},
}

@Article{bilalli17,
  author  = {Besim Bilalli and Alberto Abell{\'o} and Tom{\`a}s Aluja-Banet},
  title   = {On the predictive power of meta-features in OpenML},
  journal = {Applied Mathematics and Computer Science},
  year    = {2017},
  volume  = {27},
  pages   = {697-712},
}

@Article{howard17,
  author  = {Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
  title   = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  journal = {ArXiv},
  year    = {2017},
  volume  = {abs/1704.04861},
}

@InProceedings{tan19,
  author    = {Tan, Mingxing and Le, Quoc},
  title     = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {6105--6114},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  abstract  = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  pdf       = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url       = {http://proceedings.mlr.press/v97/tan19a.html},
}

@Misc{elsken18,
  author = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  title  = {Simple and efficient architecture search for Convolutional Neural Networks},
  year   = {2018},
  url    = {https://openreview.net/forum?id=SySaJ0xCZ},
}

@Comment{jabref-meta: databaseType:bibtex;}
